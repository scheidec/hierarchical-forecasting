---
title: "Hierarchical Time Series Forecasting"
author: "Caleb Scheidel"
date: "2018/11/30"
output:
  xaringan::moon_reader:
    css: ["mc-xaringan.css", "mc-xaringan-fonts.css"]
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

layout: true
background-color: #fafaef
<div class="my-footer"><img src="mc_logo_rectangle.png" style="height: 30px;"/></div>

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(forecast)
library(skimr)
library(tsibble)
```

---

## Motivation

- Why would we be interested in hierarchically structured time series?

---

# Forecasting

- General overview of forecasting

---

## Hierarchical Time Series

- More detail on "hierarchical" structure
  - visualize an example

---

## Optimal Combination

- Proposed by Hyndman, et al (2011)

---

## Trace Minimization

- Proposed by Hyndman, et al (2018)

---

## `hts` package

```{r}
library(hts)
```


---

## Bottom-up method

- Advantage: no information is lost due to aggregation
- Disadvantage: bottom-level data can be quite noisy and more challenging to model and forecast

---

## Top-down method

- Average historical proportions
  - Each proportion reflects the average of the historical proportions of the bottom-level series over the period _t_ relative to the total aggregate $y_{t}$
  
- Proportions of the historical averages
  - Each proportion captures the average historical value of the bottom-level series relative to the average value of the total aggregate $y_{t}$

- Forecasted proportions
  - first generate _h_-step-ahead base forecasts for all the series independently
  - at level 1 we calculate the proportion of each _h_-step-ahead base forecasts

---

## Middle-out method

- Combines bottom-up and top-down approaches
  - "middle level" is chosen and base forecasts are generated for all the series of this level and below
  - for series above the middle level, forecasts are generated using bottom up approach by aggregating the "middle level" base forecasts upwards

---

## US National Parks Visitor Data

```{r, message=FALSE}

nps <- read_csv("../data/All National Parks Visitation 1904-2016.csv") %>% 
  janitor::clean_names() %>% 
  select(park_name = unit_name,
         park_code = unit_code,
         park_type = unit_type,
         state,
         region,
         year      = year_raw,
         visitors) %>% 
  arrange(year) %>% 
  filter(year != "Total") %>% 
  mutate(year = as.numeric(year)) %>% 
  filter(park_type == "National Park") %>% 
  filter(park_name != "Denali National Preserve") %>%
  as_tsibble(index = year, key = id(park_code))
```

---

```{r}

# by park
nps %>% 
  ggplot(aes(x = year, y = visitors, colour = park_code)) +
  geom_line()
```

---

```{r}
# by state
nps %>% 
  group_by(state, year) %>% 
  summarise(visitors = sum(visitors)) %>% 
  ggplot(aes(x = year, y = visitors, colour = state)) +
  geom_line()
```

---

```{r}
# by region
nps %>% 
  group_by(region, year) %>% 
  summarise(visitors = sum(visitors)) %>% 
  ggplot(aes(x = year, y = visitors, colour = region)) +
  geom_line()
```

---

## Data

Divide the data into two parts, the first 90% of each series use as the training set for estimating parameters and the last 10% as a test set.  Are the differences in the forecasting results statistically significant?

---

## Other Methods

It is possible to forecast all series at all levels independently, but this has the undesirable consequence of the higher level forecasts not being equal to the sum of the lower level forecasts.  Adjustments can be done in an ad-hoc manner, but prediction intervals cannot be computed.
